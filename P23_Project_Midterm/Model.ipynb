{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Brick Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brickschema import Graph\n",
    "g = Graph(load_brick=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355 equipment classes found\n",
      "['https://brickschema.org/schema/Brick#AED', 'https://brickschema.org/schema/Brick#AHU', 'https://brickschema.org/schema/Brick#Absorption_Chiller', 'https://brickschema.org/schema/Brick#Access_Control_Equipment', 'https://brickschema.org/schema/Brick#Access_Reader', 'https://brickschema.org/schema/Brick#Active_Chilled_Beam', 'https://brickschema.org/schema/Brick#Air_Cooled_Chiller', 'https://brickschema.org/schema/Brick#Air_Diffuser', 'https://brickschema.org/schema/Brick#Air_Handler_Unit', 'https://brickschema.org/schema/Brick#Air_Handling_Unit', 'https://brickschema.org/schema/Brick#Air_Plenum', 'https://brickschema.org/schema/Brick#Audio_Visual_Equipment', 'https://brickschema.org/schema/Brick#Automated_External_Defibrillator', 'https://brickschema.org/schema/Brick#Automatic_Switch', 'https://brickschema.org/schema/Brick#Automatic_Tint_Window', 'https://brickschema.org/schema/Brick#Automatic_Transfer_Switch', 'https://brickschema.org/schema/Brick#BACnet_Controller', 'https://brickschema.org/schema/Brick#Backflow_Preventer_Valve', 'https://brickschema.org/schema/Brick#Baseboard_Radiator', 'https://brickschema.org/schema/Brick#Battery', 'https://brickschema.org/schema/Brick#Blind', 'https://brickschema.org/schema/Brick#Boiler', 'https://brickschema.org/schema/Brick#Booster_Fan', 'https://brickschema.org/schema/Brick#Booster_Pump', 'https://brickschema.org/schema/Brick#Branch_Selector']\n",
      "331 preferred equipment classes\n"
     ]
    }
   ],
   "source": [
    "q_all_equips = \"\"\"\n",
    "PREFIX brick: <https://brickschema.org/schema/Brick#>\n",
    "PREFIX rdfs:  <http://www.w3.org/2000/01/rdf-schema#>\n",
    "SELECT ?equip WHERE {\n",
    "    ?equip rdfs:subClassOf* brick:Equipment .\n",
    "}\n",
    "ORDER BY ?equip\n",
    "\"\"\"\n",
    "equip_list = [str(r[0]) for r in g.query(q_all_equips)]\n",
    "print(len(equip_list), \"equipment classes found\")\n",
    "print(equip_list[:25])\n",
    "\n",
    "q_preferred_equips = \"\"\"\n",
    "PREFIX brick: <https://brickschema.org/schema/Brick#>\n",
    "PREFIX rdfs:  <http://www.w3.org/2000/01/rdf-schema#>\n",
    "SELECT ?equip WHERE {\n",
    "    ?equip rdfs:subClassOf* brick:Equipment .\n",
    "    FILTER NOT EXISTS { ?equip brick:aliasOf ?alias }\n",
    "}\n",
    "ORDER BY ?equip\n",
    "\"\"\"\n",
    "pref_equips = [str(r[0]) for r in g.query(q_preferred_equips)]\n",
    "print(len(pref_equips), \"preferred equipment classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.Series(equip_list, name=\"equipment classes found\").to_csv(\"brick_equipment_list.csv\", index=False)\n",
    "pd.Series(pref_equips, name=\"preferred equipment classes\").to_csv(\"brick_prefequipment_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Pipeline\n",
    "data preprocessing, feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) load building operation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_PATH = \"Test_data.csv\"\n",
    "SHEET_NAME = 0\n",
    "RESAMPLE_RULE = \"15T\"\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_alternating(path: str, sheet_name=0):\n",
    "    if path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        df = pd.read_excel(path, sheet_name=sheet_name)\n",
    "    else:\n",
    "        df = pd.read_csv(path)\n",
    "    # 1 pair - 2 column(time, value)\n",
    "    cols = list(df.columns)\n",
    "    if len(cols) % 2 != 0:\n",
    "        raise ValueError(\"odd column, Please set (time,value)\")\n",
    "    long_parts = []\n",
    "    for i in range(0, len(cols), 2):\n",
    "        tcol, vcol = cols[i], cols[i+1]\n",
    "        sub = df[[tcol, vcol]].copy()\n",
    "        sub.columns = [\"timestamp\", \"value\"]\n",
    "        # timestamp pacing\n",
    "        sub[\"timestamp\"] = pd.to_datetime(sub[\"timestamp\"], errors=\"coerce\")\n",
    "        sub = sub.dropna(subset=[\"timestamp\"]).copy()\n",
    "        # extract value row\n",
    "        sub = sub.dropna(subset=[\"value\"], how=\"all\")\n",
    "        sub[\"point_id\"] = f\"p_{(i//2)+1}\" if vcol is None else (str(vcol) or f\"p_{(i//2)+1}\")\n",
    "        # save point_name to value column header\n",
    "        sub[\"point_name\"] = str(vcol)\n",
    "        long_parts.append(sub[[\"point_id\",\"point_name\",\"timestamp\",\"value\"]])\n",
    "    long_df = pd.concat(long_parts, ignore_index=True)\n",
    "    return long_df\n",
    "\n",
    "def detect_type_per_point(g: pd.DataFrame) -> str:\n",
    "    # {0,1} value = binary\n",
    "    vals = pd.to_numeric(g[\"value\"], errors=\"coerce\").dropna()\n",
    "    uniq = set(np.unique(vals.values))\n",
    "    if len(uniq) <= 2 and uniq.issubset({0,1}):\n",
    "        return \"binary\"\n",
    "    return \"ratio\"\n",
    "\n",
    "def preprocess_long(long_df: pd.DataFrame):\n",
    "    # change time: past to current\n",
    "    long_df = long_df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    long_df[\"value\"] = pd.to_numeric(long_df[\"value\"], errors=\"coerce\")\n",
    "    # determine type\n",
    "    types = long_df.groupby(\"point_id\").apply(detect_type_per_point).rename(\"data_type\").reset_index()\n",
    "    long_df = long_df.merge(types, on=\"point_id\", how=\"left\")\n",
    "    return long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Ratio process ----\n",
    "def build_ratio_features(long_df: pd.DataFrame, rule: str = \"15T\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    long_df: columns = [point_id, timestamp, value, data_type, ...]\n",
    "    rule   : resample freq (default '15T')\n",
    "\n",
    "    처리 순서:\n",
    "      1) time set by point\n",
    "      2) same timestamp process\n",
    "      3) generate uniform grid (date_range) after reindex\n",
    "      4) ffill/bfill + interpolate\n",
    "      5) statistical calculate\n",
    "    \"\"\"\n",
    "    def safe_autocorr(series: pd.Series, lag: int) -> float:\n",
    "        if lag <= 0 or len(series) <= lag + 1 or series.isna().all():\n",
    "            return 0.0\n",
    "        try:\n",
    "            ac = series.autocorr(lag=lag)\n",
    "            return float(ac if ac is not None else 0.0)\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    ratio = long_df[long_df[\"data_type\"] == \"ratio\"].copy()\n",
    "    feats = []\n",
    "\n",
    "    if ratio.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"point_id\",\"n\",\"missing_ratio\",\"mean\",\"std\",\"min\",\"p25\",\"median\",\"p75\",\"max\",\"iqr\",\n",
    "            \"skew\",\"kurt\",\"range\",\"zero_ratio\",\"madiff_mean\",\"max_diff\",\"pct_change_std\",\n",
    "            \"daily_cycle_strength\",\"weekly_cycle_strength\"\n",
    "        ])\n",
    "\n",
    "    daily_lag  = int(pd.Timedelta(\"24H\")  / pd.Timedelta(rule))\n",
    "    weekly_lag = int(pd.Timedelta(\"168H\") / pd.Timedelta(rule))\n",
    "\n",
    "    for pid, g in ratio.groupby(\"point_id\"):\n",
    "        g = g.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
    "        s = g.set_index(\"timestamp\")[\"value\"]\n",
    "\n",
    "        s = s.groupby(level=0).mean()\n",
    "\n",
    "        if s.index.size == 0:\n",
    "            feats.append({\n",
    "                \"point_id\": pid, \"n\": 0, \"missing_ratio\": 1.0,\n",
    "                \"mean\": 0.0, \"std\": 0.0, \"min\": 0.0, \"p25\": 0.0, \"median\": 0.0, \"p75\": 0.0, \"max\": 0.0,\n",
    "                \"iqr\": 0.0, \"skew\": 0.0, \"kurt\": 0.0, \"range\": 0.0, \"zero_ratio\": 0.0,\n",
    "                \"madiff_mean\": 0.0, \"max_diff\": 0.0, \"pct_change_std\": 0.0,\n",
    "                \"daily_cycle_strength\": 0.0, \"weekly_cycle_strength\": 0.0\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        full_index = pd.date_range(start=s.index.min(), end=s.index.max(), freq=rule)\n",
    "        s = s.reindex(full_index)\n",
    "\n",
    "        s = s.ffill().bfill().interpolate()\n",
    "\n",
    "        diffs = s.diff().abs()\n",
    "\n",
    "        # statistical calculate\n",
    "        std_ = float(s.std() if s.size > 1 else 0.0)\n",
    "        skew_ = float(0.0 if std_ in [0, np.nan] else s.skew())\n",
    "        kurt_ = float(0.0 if std_ in [0, np.nan] else s.kurt())\n",
    "\n",
    "        stats = {\n",
    "            \"point_id\": pid,\n",
    "            \"n\": int(s.shape[0]),\n",
    "            \"missing_ratio\": float(g[\"value\"].isna().mean()),\n",
    "            \"mean\": float(s.mean()),\n",
    "            \"std\": std_,\n",
    "            \"min\": float(s.min()),\n",
    "            \"p25\": float(s.quantile(0.25)),\n",
    "            \"median\": float(s.median()),\n",
    "            \"p75\": float(s.quantile(0.75)),\n",
    "            \"max\": float(s.max()),\n",
    "            \"iqr\": float(s.quantile(0.75) - s.quantile(0.25)),\n",
    "            \"skew\": skew_,\n",
    "            \"kurt\": kurt_,\n",
    "            \"range\": float(s.max() - s.min()),\n",
    "            \"zero_ratio\": float((s == 0).mean()),\n",
    "            \"madiff_mean\": float(diffs.mean() if diffs.size else 0.0),\n",
    "            \"max_diff\": float(diffs.max() if diffs.size else 0.0),\n",
    "            \"pct_change_std\": float(s.pct_change().std() if s.size > 1 else 0.0),\n",
    "            \"daily_cycle_strength\": safe_autocorr(s, daily_lag),\n",
    "            \"weekly_cycle_strength\": safe_autocorr(s, weekly_lag),\n",
    "        }\n",
    "        feats.append(stats)\n",
    "\n",
    "    return pd.DataFrame(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Binary process ----\n",
    "def build_binary_features(long_df: pd.DataFrame, rule: str = \"15T\"):\n",
    "    \"\"\"\n",
    "    long_df: columns = [point_id, timestamp, value, data_type, ...]\n",
    "    rule   : resample freq (default '15T')\n",
    "\n",
    "    output:\n",
    "      - feat_binary_events: event log based summary\n",
    "      - feat_binary_ffill : 15m resample+FFill based summary\n",
    "    \"\"\"\n",
    "    binary = long_df[long_df[\"data_type\"] == \"binary\"].copy()\n",
    "\n",
    "    ev_rows = []\n",
    "    if not binary.empty:\n",
    "        for pid, g in binary.groupby(\"point_id\"):\n",
    "            # timestamp error solve\n",
    "            g = g.dropna(subset=[\"timestamp\", \"value\"]).sort_values(\"timestamp\")\n",
    "            g = g.groupby(\"timestamp\", as_index=False).last()\n",
    "\n",
    "            if g.empty:\n",
    "                ev_rows.append({\"point_id\": pid, \"switch_count\": 0,\n",
    "                                \"avg_on_duration_s\": 0.0, \"on_ratio\": 0.0})\n",
    "                continue\n",
    "\n",
    "            v = g[\"value\"].astype(int).reset_index(drop=True)\n",
    "            t = g[\"timestamp\"].astype(\"int64\").reset_index(drop=True) // 10**9  # seconds\n",
    "\n",
    "            transitions = int((v.diff().fillna(0) != 0).sum())\n",
    "\n",
    "            # average on time\n",
    "            if (v == 1).sum() >= 2:\n",
    "                on_times = t[v == 1].diff().dropna()\n",
    "                avg_on_s = float(on_times.mean()) if not on_times.empty else 0.0\n",
    "            else:\n",
    "                avg_on_s = 0.0\n",
    "\n",
    "            on_ratio = float((v == 1).mean())\n",
    "            ev_rows.append({\n",
    "                \"point_id\": pid,\n",
    "                \"switch_count\": transitions,\n",
    "                \"avg_on_duration_s\": avg_on_s,\n",
    "                \"on_ratio\": on_ratio\n",
    "            })\n",
    "    feat_binary_events = pd.DataFrame(ev_rows)\n",
    "\n",
    "    # (B) 15m resample + FFill based summary\n",
    "    ff_rows = []\n",
    "    if not binary.empty:\n",
    "        for pid, g in binary.groupby(\"point_id\"):\n",
    "            g = g.dropna(subset=[\"timestamp\", \"value\"]).sort_values(\"timestamp\")\n",
    "            g = g.groupby(\"timestamp\", as_index=False).last()\n",
    "            if g.empty:\n",
    "                ff_rows.append({\"point_id\": pid, \"ffill_on_ratio\": 0.0, \"ffill_switch_count\": 0})\n",
    "                continue\n",
    "\n",
    "            s = g.set_index(\"timestamp\")[\"value\"].astype(int)\n",
    "            full_index = pd.date_range(start=s.index.min(), end=s.index.max(), freq=rule)\n",
    "            s = s.reindex(full_index).ffill().fillna(0).astype(int)\n",
    "            sdiff = s.diff().fillna(0).abs()\n",
    "\n",
    "            ff_rows.append({\n",
    "                \"point_id\": pid,\n",
    "                \"ffill_on_ratio\": float((s == 1).mean()),\n",
    "                \"ffill_switch_count\": int((sdiff > 0).sum()),\n",
    "            })\n",
    "    feat_binary_ffill = pd.DataFrame(ff_rows)\n",
    "\n",
    "    return feat_binary_events, feat_binary_ffill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/35/_0fhx4x939d01fyjcy5t9jtw0000gn/T/ipykernel_92929/4286527920.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  types = long_df.groupby(\"point_id\").apply(detect_type_per_point).rename(\"data_type\").reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary ===\n",
      "points: 7\n",
      "ratio points: 4\n",
      "binary points: 3\n",
      "Saved to: /Users/kim-yujin/Desktop/ALDA_Proj/outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/35/_0fhx4x939d01fyjcy5t9jtw0000gn/T/ipykernel_92929/750226528.py:33: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  daily_lag  = int(pd.Timedelta(\"24H\")  / pd.Timedelta(rule))\n",
      "/var/folders/35/_0fhx4x939d01fyjcy5t9jtw0000gn/T/ipykernel_92929/750226528.py:33: FutureWarning: 'T' is deprecated and will be removed in a future version. Please use 'min' instead of 'T'.\n",
      "  daily_lag  = int(pd.Timedelta(\"24H\")  / pd.Timedelta(rule))\n",
      "/var/folders/35/_0fhx4x939d01fyjcy5t9jtw0000gn/T/ipykernel_92929/750226528.py:34: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  weekly_lag = int(pd.Timedelta(\"168H\") / pd.Timedelta(rule))\n",
      "/var/folders/35/_0fhx4x939d01fyjcy5t9jtw0000gn/T/ipykernel_92929/750226528.py:34: FutureWarning: 'T' is deprecated and will be removed in a future version. Please use 'min' instead of 'T'.\n",
      "  weekly_lag = int(pd.Timedelta(\"168H\") / pd.Timedelta(rule))\n",
      "/var/folders/35/_0fhx4x939d01fyjcy5t9jtw0000gn/T/ipykernel_92929/750226528.py:52: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  full_index = pd.date_range(start=s.index.min(), end=s.index.max(), freq=rule)\n",
      "/var/folders/35/_0fhx4x939d01fyjcy5t9jtw0000gn/T/ipykernel_92929/750226528.py:52: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  full_index = pd.date_range(start=s.index.min(), end=s.index.max(), freq=rule)\n",
      "/var/folders/35/_0fhx4x939d01fyjcy5t9jtw0000gn/T/ipykernel_92929/750226528.py:52: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  full_index = pd.date_range(start=s.index.min(), end=s.index.max(), freq=rule)\n",
      "/opt/anaconda3/envs/ncphd/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:3065: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/var/folders/35/_0fhx4x939d01fyjcy5t9jtw0000gn/T/ipykernel_92929/750226528.py:52: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  full_index = pd.date_range(start=s.index.min(), end=s.index.max(), freq=rule)\n",
      "/var/folders/35/_0fhx4x939d01fyjcy5t9jtw0000gn/T/ipykernel_92929/734160886.py:57: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  full_index = pd.date_range(start=s.index.min(), end=s.index.max(), freq=rule)\n",
      "/var/folders/35/_0fhx4x939d01fyjcy5t9jtw0000gn/T/ipykernel_92929/734160886.py:57: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  full_index = pd.date_range(start=s.index.min(), end=s.index.max(), freq=rule)\n",
      "/var/folders/35/_0fhx4x939d01fyjcy5t9jtw0000gn/T/ipykernel_92929/734160886.py:57: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  full_index = pd.date_range(start=s.index.min(), end=s.index.max(), freq=rule)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    long_df = read_alternating(INPUT_PATH, SHEET_NAME)\n",
    "    long_df = preprocess_long(long_df)\n",
    "    long_df.to_csv(OUTPUT_DIR/\"long_timeseries.csv\", index=False)\n",
    "\n",
    "    feat_ratio = build_ratio_features(long_df)\n",
    "    feat_bin_ev, feat_bin_ff = build_binary_features(long_df)\n",
    "\n",
    "    # murge (NaN -> 0)\n",
    "    features = (feat_ratio.set_index(\"point_id\")\n",
    "                .join(feat_bin_ev.set_index(\"point_id\"), how=\"outer\")\n",
    "                .join(feat_bin_ff.set_index(\"point_id\"), how=\"outer\")).reset_index()\n",
    "    for c in features.columns:\n",
    "        if c != \"point_id\":\n",
    "            features[c] = features[c].fillna(0)\n",
    "\n",
    "    # save\n",
    "    feat_ratio.to_csv(OUTPUT_DIR/\"features_ratio.csv\", index=False)\n",
    "    feat_bin_ev.to_csv(OUTPUT_DIR/\"features_binary_events.csv\", index=False)\n",
    "    feat_bin_ff.to_csv(OUTPUT_DIR/\"features_binary_ffill.csv\", index=False)\n",
    "    features.to_csv(OUTPUT_DIR/\"features_all.csv\", index=False)\n",
    "\n",
    "    # print summary\n",
    "    print(\"=== Summary ===\")\n",
    "    print(\"points:\", long_df[\"point_id\"].nunique())\n",
    "    print(\"ratio points:\", (long_df.groupby(\"point_id\")[\"data_type\"].first()==\"ratio\").sum())\n",
    "    print(\"binary points:\", (long_df.groupby(\"point_id\")[\"data_type\"].first()==\"binary\").sum())\n",
    "    print(\"Saved to:\", OUTPUT_DIR.resolve())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: outputs/tag_candidate_sheet.csv\n"
     ]
    }
   ],
   "source": [
    "INP  = \"outputs/features_all.csv\"\n",
    "OUT  = \"outputs/tag_candidate_sheet.csv\"\n",
    "\n",
    "# unit set: \"F\",  \"C\"\n",
    "TEMP_UNIT = \"F\"   \n",
    "\n",
    "def f(r, k, d=np.nan):\n",
    "    try:\n",
    "        return float(r[k])\n",
    "    except Exception:\n",
    "        return d\n",
    "\n",
    "def to_c_if_needed(x):\n",
    "    if np.isnan(x): \n",
    "        return x\n",
    "    if TEMP_UNIT.upper() == \"F\":\n",
    "        return (x - 32.0) * 5.0/9.0\n",
    "    return x\n",
    "\n",
    "def tag_candidates(row):\n",
    "    mean  = f(row, \"mean\")\n",
    "    std   = f(row, \"std\", 0.0)\n",
    "    rng   = f(row, \"range\", 0.0)\n",
    "    zero  = f(row, \"zero_ratio\", 0.0)\n",
    "    daily = f(row, \"daily_cycle_strength\", 0.0)\n",
    "\n",
    "    tags = {}\n",
    "\n",
    "    # ---- (A) Binary determine: True - activate evidence\n",
    "    swc  = f(row, \"ffill_switch_count\", np.nan)\n",
    "    sw   = f(row, \"switch_count\", np.nan)\n",
    "    onrt = f(row, \"on_ratio\", np.nan)\n",
    "    is_binary = (\n",
    "        (not np.isnan(swc) and swc > 0) or\n",
    "        (not np.isnan(sw)  and sw  > 0) or\n",
    "        (not np.isnan(onrt) and 0.0 < onrt < 1.0)\n",
    "    )\n",
    "\n",
    "    if is_binary:\n",
    "        switches = int(swc if not np.isnan(swc) else (sw if not np.isnan(sw) else 0))\n",
    "        base = 0.65 + min(0.35, switches/50.0)\n",
    "        tags[\"status\"]  = base\n",
    "        tags[\"command\"] = base*0.85\n",
    "        tags[\"mode\"]    = base*0.80\n",
    "        return tags\n",
    "\n",
    "    # ---- (B) Ratio signal: temperature/CO2/flow rate/pressure\n",
    "    mean_c = to_c_if_needed(mean)\n",
    "\n",
    "    # Temperature band(Celcius)\n",
    "    if not np.isnan(mean_c):\n",
    "        # CHW ~ 3–14°C, Air ~ 16–32°C, HW ~ 35–75°C (rough guide)\n",
    "        if 3 <= mean_c <= 14:\n",
    "            tags[\"temperature\"] = max(tags.get(\"temperature\",0), 0.8 + 0.1*max(0,daily))\n",
    "        if 16 <= mean_c <= 32:\n",
    "            tags[\"temperature\"] = max(tags.get(\"temperature\",0), 0.75 + 0.15*max(0,daily))\n",
    "        if 35 <= mean_c <= 75:\n",
    "            tags[\"temperature\"] = max(tags.get(\"temperature\",0), 0.7 + 0.05*max(0,daily))\n",
    "\n",
    "    # CO2-like: 300~2000ppm, large fluctuation, few 0, major week cycle\n",
    "    if not np.isnan(mean) and 250 <= mean <= 2000 and rng >= 200 and zero < 0.05:\n",
    "        tags[\"co2\"] = max(tags.get(\"co2\",0), 0.7 + 0.2*max(0,daily))\n",
    "\n",
    "    # Flow-like: often 0, large variance when on \n",
    "    if zero >= 0.05 and (np.isnan(mean) or mean >= 0) and rng > 1 and (std > 0.1 or rng > 5):\n",
    "        tags[\"flow\"] = max(tags.get(\"flow\",0), 0.6 + 0.2*min(0.5, zero))\n",
    "\n",
    "    # Pressure-like: + baseline, medium fluctuation, minor week cycle\n",
    "    if not np.isnan(mean) and mean >= 50 and rng >= 10 and daily < 0.3:\n",
    "        base = 0.55 + 0.1*(rng>50) + 0.05*(std<10)\n",
    "        tags[\"pressure\"] = max(tags.get(\"pressure\",0), base)\n",
    "\n",
    "    # if Ratio suppose generic sensor \n",
    "    tags[\"sensor\"] = max(tags.get(\"sensor\",0), 0.5)\n",
    "    return tags\n",
    "\n",
    "feat = pd.read_csv(INP)\n",
    "rows=[]\n",
    "for _, r in feat.iterrows():\n",
    "    t = tag_candidates(r)\n",
    "    top = sorted(t.items(), key=lambda x: -x[1])[:3]\n",
    "    rows.append({\n",
    "        \"point_id\": r[\"point_id\"],\n",
    "        \"tag1\": top[0][0] if len(top)>0 else \"\",\n",
    "        \"tag1_score\": round(top[0][1],3) if len(top)>0 else \"\",\n",
    "        \"tag2\": top[1][0] if len(top)>1 else \"\",\n",
    "        \"tag2_score\": round(top[1][1],3) if len(top)>1 else \"\",\n",
    "        \"tag3\": top[2][0] if len(top)>2 else \"\",\n",
    "        \"tag3_score\": round(top[2][1],3) if len(top)>2 else \"\",\n",
    "        \"gold_tags\": \"\"   # tag confirmed by person(multi): 예) \"temperature,sensor\"\n",
    "    })\n",
    "\n",
    "out = pd.DataFrame(rows)\n",
    "out.to_csv(OUT, index=False)\n",
    "print(\"saved:\", OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete add label: outputs/tag_candidate_sheet_labeled.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/35/_0fhx4x939d01fyjcy5t9jtw0000gn/T/ipykernel_92929/1704461026.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'temperature' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df[\"point_id\"] == \"AHU-4 SaTmp\", \"gold_tags\"] = \"temperature\"\n"
     ]
    }
   ],
   "source": [
    "# now mannually add gold_tag\n",
    "df = pd.read_csv(\"outputs/tag_candidate_sheet.csv\")\n",
    "\n",
    "df.loc[df[\"point_id\"] == \"AHU-4 SaTmp\", \"gold_tags\"] = \"temperature\"\n",
    "df.loc[df[\"point_id\"] == \"AHU-4 ChwEnTmp\", \"gold_tags\"] = \"temperature\"\n",
    "df.loc[df[\"point_id\"] == \"AHU-4 AvgCcoilTmp\", \"gold_tags\"] = \"temperature\"\n",
    "df.loc[df[\"point_id\"] == \"AHU-4 AvgMaTmp\", \"gold_tags\"] = \"temperature\"\n",
    "df.loc[df[\"point_id\"] == \"AHU-4 HrwSts\", \"gold_tags\"] = \"status\"\n",
    "df.loc[df[\"point_id\"] == \"AHU-4 EconMd\", \"gold_tags\"] = \"status\"\n",
    "\n",
    "df.to_csv(\"outputs/tag_candidate_sheet_labeled.csv\", index=False)\n",
    "print(\"complete add label:\", \"outputs/tag_candidate_sheet_labeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 (CV mean, 2-fold): 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "df = pd.read_csv(\"outputs/tag_candidate_sheet_labeled.csv\")\n",
    "df = df[df[\"gold_tags\"].notna() & (df[\"gold_tags\"]!=\"\")].copy()\n",
    "\n",
    "# X\n",
    "feat = pd.read_csv(\"outputs/features_all.csv\").set_index(\"point_id\")\n",
    "X = feat.loc[df[\"point_id\"]].drop(columns=[\"n\"], errors=\"ignore\").values  \n",
    "\n",
    "# y (multi label)\n",
    "Y = df[\"gold_tags\"].apply(lambda s: [t.strip() for t in s.split(\",\") if t.strip()])\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(Y)\n",
    "\n",
    "clf = OneVsRestClassifier(XGBClassifier(\n",
    "    n_estimators=400, max_depth=6, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, tree_method=\"hist\", random_state=42\n",
    "))\n",
    "\n",
    "# simple CV\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "major = y.argmax(axis=1)\n",
    "\n",
    "counts = np.bincount(major)\n",
    "min_class = counts[counts > 0].min() if counts.size else 0\n",
    "\n",
    "desired_splits = 5\n",
    "n_splits = max(2, min(desired_splits, int(min_class)))  \n",
    "\n",
    "clf = OneVsRestClassifier(XGBClassifier(\n",
    "    n_estimators=400, max_depth=6, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    tree_method=\"hist\", random_state=42\n",
    "))\n",
    "\n",
    "scores = []\n",
    "\n",
    "if n_splits >= 2:\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    for tr, te in skf.split(X, major):\n",
    "        clf.fit(X[tr], y[tr])\n",
    "        pred = (clf.predict_proba(X[te]) > 0.5).astype(int)\n",
    "        scores.append(f1_score(y[te], pred, average=\"macro\", zero_division=0))\n",
    "    print(f\"Macro F1 (CV mean, {n_splits}-fold):\", float(np.mean(scores)))\n",
    "else:\n",
    "    X_tr, X_te, y_tr, y_te, major_tr, major_te = train_test_split(\n",
    "        X, y, major, test_size=0.25, random_state=42,\n",
    "        stratify=major if len(np.unique(major)) > 1 else None\n",
    "    )\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    pred = (clf.predict_proba(X_te) > 0.5).astype(int)\n",
    "    print(\"Macro F1 (holdout):\", f1_score(y_te, pred, average=\"macro\", zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncphd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
